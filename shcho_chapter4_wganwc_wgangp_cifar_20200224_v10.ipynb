{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "shcho-chapter4-wganwc-wgangp-cifar-20200224-v10.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMRyGYyn9rZfTSTwLi8NSxd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2c61983489814cb8a5db7f8c51a8ea16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_4353285d3f7a4e4b8af136d701ae6ce7",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_5f47e6fd519947aea65dc6f5c4eeb972",
              "IPY_MODEL_fd2ec5f3eb524a758275da127432c75a"
            ]
          }
        },
        "4353285d3f7a4e4b8af136d701ae6ce7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5f47e6fd519947aea65dc6f5c4eeb972": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_7d8d9109cc154bd9b85fbcbf9175f57e",
            "_dom_classes": [],
            "description": "",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 47,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 47,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_13757056fc534d0bababb5f755b62253"
          }
        },
        "fd2ec5f3eb524a758275da127432c75a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_340f7b7d10fe43a8a69b05834d59e3c2",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100% 47/47 [00:00&lt;00:00, 771.00it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b1231f2c3fdd41239518f987e0693548"
          }
        },
        "7d8d9109cc154bd9b85fbcbf9175f57e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "13757056fc534d0bababb5f755b62253": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "340f7b7d10fe43a8a69b05834d59e3c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b1231f2c3fdd41239518f987e0693548": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JungOhLee/GDL_pytorch/blob/shcho/shcho_chapter4_wganwc_wgangp_cifar_20200224_v10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Mz5yuU5aSmJ",
        "colab_type": "code",
        "outputId": "c923af02-4c74-4861-d5f8-0cb8a13cd756",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "source": [
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OVrkRlIEaa_n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch import nn\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.optim as optim\n",
        "import  tarfile\n",
        "from torchsummary import summary as ptsum\n",
        "import torch.autograd as autograd\n",
        "import png\n",
        "\n",
        "from PIL import Image\n",
        "from torch.autograd import Variable"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPiGihCWbsIC",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-4ZWoSB2bpRd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oM4EDvU4aaDj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import collections\n",
        "from torch.autograd import Variable\n",
        "import warnings\n",
        "warnings.filterwarnings(action='ignore')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hf1kVGW6abBT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tqdm import tqdm_notebook as tqdm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9qTYkwRsZjaC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert(data):\n",
        "    if isinstance(data, bytes):  return data.decode('ascii')\n",
        "    if isinstance(data, dict):   return dict(map(convert, data.items()))\n",
        "    if isinstance(data, tuple):  return map(convert, data)\n",
        "    return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-A8xzyynY2U9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def unpickle(file):\n",
        "    import pickle\n",
        "    with open(file, 'rb') as fo:\n",
        "        dict = pickle.load(fo, encoding='bytes')\n",
        "    return dict"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KrlHmBf5ZbLr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "DATA_HOME_DIR = './drive/My Drive/Colab Notebooks/mlstudy/gan/gdl_code/data/cifar-10-batches-py/'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OeB1T3jCavqF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_files = [item for item in os.listdir(DATA_HOME_DIR) if item not in ['readme.html', 'batches.meta']]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xA7L8kkZoov",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_dict_list = []\n",
        "for item in data_files:\n",
        "  DATA_DIR = DATA_HOME_DIR + item\n",
        "  data_dict  = convert(unpickle(DATA_DIR))\n",
        "\n",
        "  data_dict_list.append(data_dict)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WhxoR29S_-jc",
        "colab_type": "code",
        "outputId": "f0b62732-11ef-4b82-bb10-4de346a239d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "len(data_dict_list)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ka1X_Hqyaa9p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "_x_train = []\n",
        "for item in data_dict_list:\n",
        "  for _label, _data in zip(item['labels'], item['data']):\n",
        "    if _label == 7:\n",
        "      _x_train.append(_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBmV_r0zAIun",
        "colab_type": "code",
        "outputId": "b82032b7-9b5c-44c9-9b31-7e5107e89be2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "len(_x_train)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6000"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vY7SjB2Yd3oR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "_x_train_stacked = np.stack(_x_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LOZ9DLLLd7qi",
        "colab_type": "code",
        "outputId": "9dff4d7d-cf34-4c8a-cc06-6251963b9e78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "_x_train_stacked.shape"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6000, 3072)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sa3wGISwcyzo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = _x_train_stacked.reshape(-1,3,32,32)/255\n",
        "for_figure_x_train = x_train.transpose([0,2,3,1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kevb_ct5aa7v",
        "colab_type": "code",
        "outputId": "9d832aaf-c2a7-4043-d1ab-4a5ae9e388e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "for_figure_x_train.shape"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(6000, 32, 32, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NbpkQZH5aa5Z",
        "colab_type": "code",
        "outputId": "1e76e906-9036-4c54-8244-cb6c8a6cdfa1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        }
      },
      "source": [
        "plt.imshow(for_figure_x_train[3,:,:,:])"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f8694dbcba8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD5CAYAAADhukOtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQiSCnF2IRJCzC5EIcnYhEkHOLkQi/F+BVYdik4haMgAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kh68_i5Uaa3P",
        "colab_type": "text"
      },
      "source": [
        "## Architecture"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_MzSex5aa2G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Discriminator_PT(nn.Module): # GAN Pytorch\n",
        "    def __init__(self\n",
        "        , input_dim\n",
        "        , discriminator_conv_filters\n",
        "        , discriminator_conv_kernel_size\n",
        "        , discriminator_conv_strides\n",
        "        , discriminator_batch_norm_momentum\n",
        "        , discriminator_activation\n",
        "        , discriminator_dropout_rate\n",
        "        , discriminator_learning_rate\n",
        "        , optimiser\n",
        "        , z_dim\n",
        "        , activation_function\n",
        "        ):\n",
        "        \n",
        "        super(Discriminator_PT, self).__init__()\n",
        "\n",
        "        self.name = 'gan'\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "        \n",
        "        ## discriminator\n",
        "        self.discriminator_conv_filters_input = [input_dim[0]] + discriminator_conv_filters[:-1]\n",
        "        self.discriminator_conv_filters_output = discriminator_conv_filters\n",
        "        self.discriminator_conv_kernel_size = discriminator_conv_kernel_size\n",
        "        self.discriminator_conv_strides = discriminator_conv_strides\n",
        "        self.discriminator_batch_norm_momentum = discriminator_batch_norm_momentum\n",
        "        self.discriminator_activation = discriminator_activation\n",
        "        self.discriminator_dropout_rate = discriminator_dropout_rate\n",
        "        self.discriminator_learning_rate = discriminator_learning_rate\n",
        "        self.activation_function = activation_function\n",
        "\n",
        "        \n",
        "        self.z_dim = z_dim\n",
        "\n",
        "        self.n_layers_discriminator = len(discriminator_conv_filters)\n",
        "\n",
        "        # initializer\n",
        "        self.weight_init = torch.normal(mean=torch.Tensor([0.]), std=0.02)\n",
        "\n",
        "        self.d_losses = []\n",
        "        self.g_losses = []\n",
        "\n",
        "        self.epoch = 0\n",
        "        \n",
        "        self.discriminator_conv_layers = nn.Sequential()\n",
        "        self.discriminator_output_shape = collections.OrderedDict()\n",
        "        \n",
        "        self.discriminator_output_shape['Input'] = self.input_dim\n",
        "\n",
        "        self._build_discriminator()\n",
        "\n",
        "    def _build_discriminator(self):\n",
        "\n",
        "        print('Build Discriminator')\n",
        "        current_layer_name = 'Input'\n",
        "        current_input_shape = self.discriminator_output_shape[current_layer_name]\n",
        "        \n",
        "        for i in range(self.n_layers_discriminator):\n",
        "            \n",
        "            current_filter, input_H, input_W = current_input_shape\n",
        "            current_layer_name = 'Layer {} Conv2d'.format(i)\n",
        "            \n",
        "            kernel_H, kernel_W = self.discriminator_conv_kernel_size[i]\n",
        "            stride_H, stride_W  = self.discriminator_conv_strides[i]\n",
        "            \n",
        "            _output_H = int((input_H-kernel_H)/stride_H) + 1 # int 논리 맞는지 확인\n",
        "            _output_W = int((input_W-kernel_W)/stride_W) + 1\n",
        "            \n",
        "            target_output_H = np.ceil(input_H/stride_H)\n",
        "            target_output_W = np.ceil(input_W/stride_W)\n",
        "            \n",
        "            if target_output_H != _output_H:\n",
        "                padding_H = int(np.ceil(((target_output_H-1)*stride_H-input_H+kernel_H)/2))\n",
        "            else:\n",
        "                padding_H = 0\n",
        "                \n",
        "            if target_output_W != _output_W:\n",
        "                padding_W = int(np.ceil(((target_output_W-1)*stride_W-input_W+kernel_W)/2))\n",
        "            else:\n",
        "                padding_W = 0\n",
        "            \n",
        "            output_H = int((input_H-kernel_H+2*padding_H)/stride_H) + 1\n",
        "            output_W = int((input_W-kernel_W+2*padding_W)/stride_W) + 1\n",
        "            \n",
        "            print(output_H, output_W)\n",
        "            \n",
        "            self.discriminator_output_shape[current_layer_name] = [self.discriminator_conv_filters_output[i], output_H, output_W]\n",
        "            current_input_shape = self.discriminator_output_shape[current_layer_name]\n",
        "            \n",
        "            conv = nn.Conv2d(\n",
        "                        self.discriminator_conv_filters_input[i]\n",
        "                        , self.discriminator_conv_filters_output[i]\n",
        "                        , kernel_size = self.discriminator_conv_kernel_size[i]\n",
        "                        , stride = self.discriminator_conv_strides[i]\n",
        "                        , padding = (padding_H, padding_W)\n",
        "#                         , kernel_initializer = self.weight_init\n",
        "                        )\n",
        "            # conv.weight.data.apply_(self.weight_init)\n",
        "\n",
        "            self.discriminator_conv_layers.add_module(\n",
        "                current_layer_name,\n",
        "                conv\n",
        "            )\n",
        "\n",
        "            if self.discriminator_batch_norm_momentum and i > 0:\n",
        "                self.discriminator_conv_layers.add_module('Layer {} BatchNorm2d'.format(i), \n",
        "                                            nn.BatchNorm2d(self.discriminator_conv_filters_output[i],\n",
        "                                                          momentum=self.discriminator_batch_norm_momentum))\n",
        "\n",
        "            self.discriminator_conv_layers.add_module('Layer {} LeakyReLU'.format(i), nn.LeakyReLU())\n",
        "\n",
        "            if self.discriminator_dropout_rate:\n",
        "                self.discriminator_conv_layers.add_module('Layer {} Dropout2d'.format(i), nn.Dropout2d(p=self.discriminator_dropout_rate))\n",
        "            \n",
        "        self.discriminator_cnn_output_shape = current_input_shape\n",
        "        \n",
        "        self.discriminator_flattened_shape = np.prod(self.discriminator_cnn_output_shape)\n",
        "\n",
        "        \n",
        "        ## Flatten 층 생성\n",
        "        ## 역시 이 전에 reshaping은 forward에서\n",
        "        self.discriminator_flattened = nn.Linear(self.discriminator_flattened_shape, 1)\n",
        "        self.Sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        \n",
        "        x = self.discriminator_conv_layers(x)\n",
        "        x = x.reshape(x.size()[0],-1)\n",
        "        \n",
        "#         print(x.shape)\n",
        "        \n",
        "        x = self.discriminator_flattened(x)\n",
        "\n",
        "        if self.activation_function == 'Sigmoid':\n",
        "          x = self.Sigmoid(x)\n",
        "        \n",
        "        \n",
        "        return x\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cbiH1Kx3aayt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## generator 는 똑같다.\n",
        "\n",
        "class Generator_PT(nn.Module): # GAN Pytorch\n",
        "    def __init__(self\n",
        "        , generator_initial_dense_layer_size\n",
        "        , input_dim\n",
        "        , generator_upsample\n",
        "        , generator_conv_filters\n",
        "        , generator_conv_kernel_size\n",
        "        , generator_conv_strides\n",
        "        , generator_batch_norm_momentum\n",
        "        , generator_activation\n",
        "        , generator_dropout_rate\n",
        "        , generator_learning_rate\n",
        "        , optimiser\n",
        "        , z_dim\n",
        "        , discriminator_output_shape\n",
        "        , discriminator_cnn_output_shape\n",
        "        ):\n",
        "        \n",
        "        super(Generator_PT, self).__init__()\n",
        "\n",
        "        # Necessary information from discriminator\n",
        "        self.discriminator_output_shape = discriminator_output_shape\n",
        "        self.discriminator_cnn_output_shape = discriminator_cnn_output_shape\n",
        "        \n",
        "        self.name = 'gan'\n",
        "        self.input_dim = input_dim\n",
        "        \n",
        "        ## generator\n",
        "        self.generator_initial_dense_layer_size = generator_initial_dense_layer_size\n",
        "        self.generator_upsample = generator_upsample\n",
        "        self.generator_conv_filters_input = [self.generator_initial_dense_layer_size[0]] + generator_conv_filters[:-1]\n",
        "        self.generator_conv_filters_output = generator_conv_filters\n",
        "        self.generator_conv_kernel_size = generator_conv_kernel_size\n",
        "        self.generator_conv_strides = generator_conv_strides\n",
        "        self.generator_batch_norm_momentum = generator_batch_norm_momentum\n",
        "        self.generator_activation = generator_activation\n",
        "        self.generator_dropout_rate = generator_dropout_rate\n",
        "        self.generator_learning_rate = generator_learning_rate\n",
        "        \n",
        "        self.z_dim = z_dim\n",
        "\n",
        "        self.n_layers_generator = len(generator_conv_filters)\n",
        "\n",
        "        # initializer\n",
        "        self.weight_init = torch.normal(mean=torch.Tensor([0.]), std=0.02)\n",
        "\n",
        "        self.d_losses = []\n",
        "        self.g_losses = []\n",
        "\n",
        "        self.epoch = 0\n",
        "                \n",
        "        self.generator_conv_layers = nn.Sequential()\n",
        "        self.generator_output_shape = collections.OrderedDict()\n",
        "        \n",
        "        self.generator_initial_layers = nn.Sequential()\n",
        "        \n",
        "        self.generator_output_shape['Input'] = self.input_dim\n",
        "        self.generator_output_shape['Input'] = self.generator_initial_dense_layer_size\n",
        "\n",
        "\n",
        "        self._build_generator()\n",
        "\n",
        "    def get_activation(self, activation):\n",
        "        if activation == 'leaky_relu':\n",
        "            layer = LeakyReLU(alpha = 0.2)\n",
        "        else:\n",
        "            layer = Activation(activation)\n",
        "        return layer\n",
        "\n",
        "    def _build_generator(self):\n",
        "\n",
        "        print('Build Generator')\n",
        "        ### THE generator\n",
        "\n",
        "        self.generator_initial_layers.add_module('Layer Generator Input', \n",
        "                                            nn.Linear(self.z_dim, np.prod(self.generator_initial_dense_layer_size)))\n",
        "\n",
        "        self.generator_initial_layers.add_module('Layer Generator Input LeakyReLU', nn.LeakyReLU())\n",
        "        \n",
        "        if self.generator_dropout_rate:\n",
        "            self.generator_initial_layers.add_module('Layer Generator Input Dropout2d', nn.Dropout2d(p=self.generator_dropout_rate))\n",
        "            \n",
        "        # 이건 forward가서\n",
        "#         x = Reshape(self.generator_initial_dense_layer_size)(x)\n",
        "        \n",
        "        current_layer_name = 'Input'\n",
        "        current_input_shape = self.generator_output_shape[current_layer_name]\n",
        "        \n",
        "        discriminator_conv_layer_name_list = []\n",
        "        \n",
        "#         print(self.discriminator_output_shape)\n",
        "        \n",
        "        for item in self.discriminator_output_shape:\n",
        "            discriminator_conv_layer_name_list.append(item)\n",
        "        discriminator_conv_layer_name_list.reverse()\n",
        "\n",
        "#         print(discriminator_conv_layer_name_list)\n",
        "        for i, discriminator_layer_name in zip(range(self.n_layers_generator), discriminator_conv_layer_name_list[1:]):\n",
        "            \n",
        "#             print(discriminator_layer_name)\n",
        "            \n",
        "            discriminator_layer_shape = self.discriminator_output_shape[discriminator_layer_name]\n",
        "            \n",
        "            current_filter, input_H, input_W = discriminator_layer_shape\n",
        "            current_layer_name = 'Layer {} Conv2d'.format(i)\n",
        "\n",
        "        \n",
        "            if self.generator_upsample[i] == 2:\n",
        "                \n",
        "                current_filter, input_H, input_W = current_input_shape\n",
        "                \n",
        "                input_H = input_H*self.generator_upsample[i]\n",
        "                input_W = input_W*self.generator_upsample[i]\n",
        "                \n",
        "                current_layer_name = 'Layer {} Conv2d'.format(i)\n",
        "\n",
        "                kernel_H, kernel_W = self.generator_conv_kernel_size[i]\n",
        "                stride_H, stride_W  = self.generator_conv_strides[i]\n",
        "\n",
        "                _output_H = int((input_H-kernel_H)/stride_H) + 1\n",
        "                _output_W = int((input_W-kernel_W)/stride_W) + 1\n",
        "\n",
        "                target_output_H = int(input_H/stride_H)\n",
        "                target_output_W = int(input_W/stride_W)\n",
        "\n",
        "                if target_output_H != _output_H:\n",
        "                    padding_H = int(np.ceil(((target_output_H-1)*stride_H-input_H+kernel_H)/2))\n",
        "                else:\n",
        "                    padding_H = 0\n",
        "\n",
        "                if target_output_W != _output_W:\n",
        "                    padding_W = int(np.ceil(((target_output_W-1)*stride_W-input_W+kernel_W)/2))\n",
        "                else:\n",
        "                    padding_W = 0\n",
        "\n",
        "                output_H = int((input_H-kernel_H+2*padding_H)/stride_H) + 1\n",
        "                output_W = int((input_W-kernel_W+2*padding_W)/stride_W) + 1\n",
        "                \n",
        "                self.generator_output_shape[current_layer_name] = [self.generator_conv_filters_output[i], output_H, output_W]\n",
        "                current_input_shape = self.generator_output_shape[current_layer_name]\n",
        "                \n",
        "\n",
        "                self.generator_conv_layers.add_module('Layer {} Upsample'.format(i),\n",
        "                                            nn.Upsample(scale_factor=self.generator_upsample[i])\n",
        "                                           )\n",
        "                \n",
        "\n",
        "                self.generator_conv_layers.add_module(\n",
        "                    current_layer_name,\n",
        "                    nn.Conv2d(\n",
        "                            self.generator_conv_filters_input[i]\n",
        "                            , self.generator_conv_filters_output[i]\n",
        "                            , kernel_size = self.generator_conv_kernel_size[i]\n",
        "                            , stride = self.generator_conv_strides[i]\n",
        "                            , padding = (padding_H, padding_W)\n",
        "    #                         , kernel_initializer = self.weight_init\n",
        "                            )\n",
        "                )\n",
        "                \n",
        "            else:\n",
        "                \n",
        "                # defining size\n",
        "                current_filter, input_H, input_W = current_input_shape\n",
        "                current_layer_name = 'Layer {} Conv2d'.format(i)\n",
        "\n",
        "                kernel_H, kernel_W = self.generator_conv_kernel_size[i]\n",
        "                stride_H, stride_W = self.generator_conv_strides[i]\n",
        "\n",
        "                target_output_H = max(input_H*stride_H,discriminator_layer_shape[1])\n",
        "                target_output_W = max(input_W*stride_W,discriminator_layer_shape[2])\n",
        "\n",
        "                padding_output_padding_H = (input_H - 1) * stride_H + (kernel_H - 1) + 1 - target_output_H\n",
        "                padding_output_padding_W = (input_W - 1) * stride_W + (kernel_W - 1) + 1 - target_output_W\n",
        "                \n",
        "                if padding_output_padding_H%2==0:\n",
        "                    output_padding_H = 0\n",
        "                    padding_H = int(padding_output_padding_H/2)\n",
        "                else:\n",
        "                    output_padding_H = max(stride_H-1,0)\n",
        "                    padding_H = int((padding_output_padding_H+output_padding_H)/2)\n",
        "                    \n",
        "                if padding_output_padding_W%2==0:\n",
        "                    output_padding_W = 0\n",
        "                    padding_W = int(padding_output_padding_W/2)\n",
        "                else:\n",
        "                    output_padding_W = max(stride_W-1,0)\n",
        "                    padding_W = int((padding_output_padding_W+output_padding_W)/2)\n",
        "\n",
        "                # padding_H = max(0, padding_H)\n",
        "                # padding_W = max(0, padding_W)\n",
        "                    \n",
        "                output_H = (input_H-1)*stride_H + (kernel_H-1)+1 - 2 * padding_H + output_padding_H\n",
        "                output_W = (input_W-1)*stride_W + (kernel_W-1)+1 - 2 * padding_W + output_padding_W\n",
        "\n",
        "                self.generator_output_shape[current_layer_name] = [self.generator_conv_filters_output[i], output_H, output_W]\n",
        "                current_input_shape = self.generator_output_shape[current_layer_name]\n",
        "                \n",
        "                # print('\\n')\n",
        "                # print(output_H, target_output_H, padding_H, output_padding_H, stride_H, kernel_H)\n",
        "\n",
        "                # self.output_shape[current_layer_name] = [self.generator_conv_filters_output[i], output_H, output_W]\n",
        "                # current_input_shape = self.output_shape[current_layer_name]\n",
        "\n",
        "                self.generator_conv_layers.add_module(current_layer_name,\n",
        "                                nn.ConvTranspose2d(self.generator_conv_filters_input[i]\n",
        "                                        , self.generator_conv_filters_output[i]\n",
        "                                        , kernel_size = self.generator_conv_kernel_size[i]\n",
        "                                        , stride = self.generator_conv_strides[i]\n",
        "                                        , padding = (padding_H, padding_W)\n",
        "                                        , output_padding = (output_padding_H, output_padding_W)\n",
        "                                        ))\n",
        "\n",
        "            print(output_H, output_W, padding_H, padding_W)\n",
        "                \n",
        "            if i < self.n_layers_generator - 1:\n",
        "\n",
        "                if self.generator_batch_norm_momentum:\n",
        "                    self.generator_conv_layers.add_module('Layer {} BatchNorm2d'.format(i), \n",
        "                                                nn.BatchNorm2d(self.generator_conv_filters_output[i],\n",
        "                                                              momentum=self.generator_batch_norm_momentum))\n",
        "\n",
        "                self.generator_conv_layers.add_module('Layer {} LeakyReLU'.format(i), nn.LeakyReLU())\n",
        "                    \n",
        "                \n",
        "            # else:\n",
        "\n",
        "            self.tanh = nn.Tanh()\n",
        "\n",
        "\n",
        "        # 이 부분도 forward로\n",
        "#         generator_output = x\n",
        "\n",
        "#         self.generator = Model(generator_input, generator_output)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        \n",
        "        \n",
        "        x = self.generator_initial_layers(x)\n",
        "        # print(x.shape)\n",
        "        \n",
        "        x = x.reshape([x.size()[0]]+list(self.generator_initial_dense_layer_size))\n",
        "        \n",
        "        x = self.generator_conv_layers(x)\n",
        "        x = self.tanh(x)\n",
        "        \n",
        "        return x\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nH6DOaSGfxV9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "z_dim = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QmFgaVFdjvPD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_dim = (3, 32, 32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gEKYgsy-Aprk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "activation_function = 'Sigmoid'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "668V6wCO-l2V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "activation_function = 'WGAN'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7sst1KTwslGS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "activation_function = 'WGANGP'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wHizhS16xJaP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if activation_function == 'Sigmoid':\n",
        "  discriminator_learning_rate = 0.0008\n",
        "  generator_learning_rate = 0.0004\n",
        "else:\n",
        "  discriminator_learning_rate = 0.00005\n",
        "  generator_learning_rate = 0.00005"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HTFGVZL7tNRN",
        "colab_type": "text"
      },
      "source": [
        "device = 'cpu'"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cnyHdhg6aauk",
        "colab_type": "code",
        "outputId": "f7cb3f6f-4860-43db-bfb9-202ae560b682",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "Discriminator = Discriminator_PT(\n",
        "    input_dim = input_dim\n",
        "    , discriminator_conv_filters = [32,64,128,128]\n",
        "    , discriminator_conv_kernel_size = [(5,5),(5,5),(5,5),(5,5)]\n",
        "    , discriminator_conv_strides = [(2,2),(2,2),(2,2),(1,1)]\n",
        "    , discriminator_batch_norm_momentum = 0.5\n",
        "    , discriminator_activation = 'relu'\n",
        "    , discriminator_dropout_rate = 0.4\n",
        "    , discriminator_learning_rate = discriminator_learning_rate\n",
        "    , optimiser = 'rmsprop'\n",
        "    , z_dim = z_dim\n",
        "    , activation_function = activation_function\n",
        "    ).to(device)"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Build Discriminator\n",
            "16 16\n",
            "8 8\n",
            "4 4\n",
            "4 4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U8z_nYgVlEAp",
        "colab_type": "code",
        "outputId": "1e017762-c54e-44d3-fd96-66d36e2a56c6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "Generator = Generator_PT(\n",
        "    input_dim = input_dim\n",
        "    , generator_initial_dense_layer_size = (128, 4, 4)\n",
        "    , generator_upsample = [2,2, 2, 1]\n",
        "    , generator_conv_filters = [128,64, 64,3]\n",
        "    , generator_conv_kernel_size = [(5,5),(5,5),(5,5),(5,5)]\n",
        "    , generator_conv_strides = [(1,1),(1,1), (1,1), (1,1)]\n",
        "    , generator_batch_norm_momentum = 0.8\n",
        "    , generator_dropout_rate = 0.25\n",
        "    , generator_activation = 'relu'\n",
        "    , generator_learning_rate = generator_learning_rate\n",
        "    , optimiser = 'rmsprop'\n",
        "    , z_dim = z_dim\n",
        "    , discriminator_output_shape = Discriminator.discriminator_output_shape\n",
        "    , discriminator_cnn_output_shape = Discriminator.discriminator_cnn_output_shape\n",
        "    ).to(device)"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Build Generator\n",
            "8 8 2 2\n",
            "16 16 2 2\n",
            "32 32 2 2\n",
            "32 32 2 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHav9Fi7aaqI",
        "colab_type": "text"
      },
      "source": [
        "## 학습"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPfH9Z7ollkZ",
        "colab_type": "code",
        "outputId": "a0afbdf4-3b86-49cf-9d54-1ce1cd22063e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "Discriminator.train()\n",
        "Generator.train()\n",
        "print('Train mode')"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train mode\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-RB91ndpaakV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Binary Cross Entropy loss\n",
        "criterion = nn.BCELoss().to(device)\n",
        "\n",
        "# 생성자의 매개 변수를 최적화하는 Adam optimizer\n",
        "G_optimizer = optim.RMSprop(Generator.parameters(), lr=Generator.generator_learning_rate)\n",
        "# 구분자의 매개 변수를 최적화하는 Adam optimizer\n",
        "D_optimizer = optim.RMSprop(Discriminator.parameters(), \n",
        "                            lr=Discriminator.discriminator_learning_rate)\n",
        "\n",
        "# optim.RMSprop\n",
        "# optim.Adam"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cRKWgdX8aaiR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train_pt = torch.from_numpy(x_train).float().to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ShnanfPCaagf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPOCHS = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k-qCBwrtaae0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 128"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kN8tJ4Iob9Uz",
        "colab_type": "code",
        "outputId": "f7eceb06-6e0a-40a5-cbe0-abeca3f83ecf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67,
          "referenced_widgets": [
            "2c61983489814cb8a5db7f8c51a8ea16",
            "4353285d3f7a4e4b8af136d701ae6ce7",
            "5f47e6fd519947aea65dc6f5c4eeb972",
            "fd2ec5f3eb524a758275da127432c75a",
            "7d8d9109cc154bd9b85fbcbf9175f57e",
            "13757056fc534d0bababb5f755b62253",
            "340f7b7d10fe43a8a69b05834d59e3c2",
            "b1231f2c3fdd41239518f987e0693548"
          ]
        }
      },
      "source": [
        "for epoch in range(EPOCHS):\n",
        "    \n",
        "    for real_data in tqdm(DataLoader(x_train_pt, batch_size = batch_size, shuffle=True)):\n",
        "      pass"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2c61983489814cb8a5db7f8c51a8ea16",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, max=47), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4Y5GbhgdQ69",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPOCHS = 5000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2hkQJfgqi9x",
        "colab_type": "text"
      },
      "source": [
        "## activation_function\n",
        "\n",
        "이것이 'None'이면 WGAN이다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6A2kcB72sflL",
        "colab_type": "code",
        "outputId": "ca9fdd4a-c08b-441a-bd09-e92236f3df84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(activation_function)"
      ],
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WGANGP\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OlfC8MkHyiDo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "CLIP_THRESHOLD = 0.01"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BywL244r9xEu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def compute_gradient_penalty(D, real_samples, fake_samples):\n",
        "    \"\"\"Calculates the gradient penalty loss for WGAN GP\"\"\"\n",
        "    # Random weight term for interpolation between real and fake samples\n",
        "    alpha = torch.from_numpy(np.random.random((real_samples.size(0), 1, 1, 1))).to(device)\n",
        "    # Get random interpolation between real and fake samples\n",
        "    interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples)).requires_grad_(True)\n",
        "    d_interpolates = D(interpolates.float())\n",
        "    fake = torch.ones(real_samples.shape[0], 1).to(device)\n",
        "    # Get gradient w.r.t. interpolates\n",
        "    gradients = autograd.grad(\n",
        "        outputs=d_interpolates,\n",
        "        inputs=interpolates,\n",
        "        grad_outputs=fake,\n",
        "        create_graph=True,\n",
        "        retain_graph=True,\n",
        "        only_inputs=True,\n",
        "    )[0]\n",
        "    gradients = gradients.view(gradients.size(0), -1)\n",
        "    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
        "    return gradient_penalty"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LJk3gkNsxKc",
        "colab_type": "text"
      },
      "source": [
        "```\n",
        "D, real_samples, fake_samples = Discriminator, real_data.data, fake_data.data\n",
        "\n",
        "alpha = torch.from_numpy(np.random.random((real_samples.size(0), 1, 1, 1))).to(device)\n",
        "# Get random interpolation between real and fake samples\n",
        "interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples)).requires_grad_(True)\n",
        "d_interpolates = D(interpolates.float())\n",
        "fake = torch.ones(real_samples.shape[0], 1).to(device)\n",
        "# Get gradient w.r.t. interpolates\n",
        "gradients = autograd.grad(\n",
        "    outputs=d_interpolates,\n",
        "    inputs=interpolates,\n",
        "    grad_outputs=fake,\n",
        "    create_graph=True,\n",
        "    retain_graph=True,\n",
        "    only_inputs=True,\n",
        ")[0]\n",
        "gradients = gradients.view(gradients.size(0), -1)\n",
        "gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2jjnBXDa2iPQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if activation_function == 'Sigmoid':\n",
        "  discriminator_train_number = 1\n",
        "else:\n",
        "  discriminator_train_number = 5"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_BvmlzsHzwZu",
        "colab_type": "text"
      },
      "source": [
        "https://github.com/eriklindernoren/PyTorch-GAN/blob/master/implementations/wgan_gp/wgan_gp.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5PlYEQ5sVSrt",
        "colab_type": "text"
      },
      "source": [
        "## Clipping 시키는 예시\n",
        "\n",
        "https://github.com/Zeleni9/pytorch-wgan/blob/master/models/wgan_clipping.py"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RcDDH6I-BECW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loss weight for gradient penalty\n",
        "lambda_gp = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ReZP9GpHKZk",
        "colab_type": "code",
        "outputId": "b1981ab9-4d9a-472f-ba49-60e32751afc7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "print(activation_function)"
      ],
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WGANGP\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34BcQXZNsTtn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "restart_epoch = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H_tf6blIaacZ",
        "colab_type": "code",
        "outputId": "794c86ed-48df-47b8-9e21-2830e201e120",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "try:\n",
        "  restart_epoch = epoch\n",
        "except:\n",
        "  restart_epoch = 0\n",
        "\n",
        "for epoch in range(restart_epoch, EPOCHS):\n",
        "  \n",
        "  total_D_loss = 0\n",
        "  total_G_loss = 0\n",
        "  \n",
        "  batch_count_for_generator = 0\n",
        "  # for real_data in tqdm(DataLoader(x_train_pt, batch_size = batch_size, shuffle=True)):\n",
        "  for real_data in DataLoader(x_train_pt, batch_size = batch_size, shuffle=True):\n",
        "    # print(batch_count_for_generator)\n",
        "    real_data_shape = real_data.shape[0]\n",
        "\n",
        "    if real_data_shape!=batch_size:\n",
        "      continue\n",
        "\n",
        "    target_real = Variable(torch.ones(real_data_shape, 1)).float().to(device)\n",
        "\n",
        "    ## 판별자\n",
        "    for p in Discriminator.parameters():\n",
        "      p.requires_grad = True  # to avoid computation\n",
        "    for p in Generator.parameters():\n",
        "      p.requires_grad = False  # to avoid computation\n",
        "    D_optimizer.zero_grad()\n",
        "    Discriminator.zero_grad()\n",
        "\n",
        "    if activation_function == 'WGAN':\n",
        "      for p in Discriminator.parameters():\n",
        "        p.data.clamp_(-CLIP_THRESHOLD, CLIP_THRESHOLD)\n",
        "\n",
        "    D_result_from_real = Discriminator(real_data)\n",
        "    z = (torch.randn((real_data_shape, z_dim))).to(device)\n",
        "    fake_data = Generator(z)\n",
        "    D_result_from_fake = Discriminator(fake_data)\n",
        "    \n",
        "    if activation_function == 'Sigmoid':\n",
        "      target_fake = Variable(torch.zeros(real_data_shape, 1)).float().to(device)\n",
        "\n",
        "      D_loss_real = criterion(D_result_from_real, target_real)\n",
        "      D_loss_fake = criterion(D_result_from_fake, target_fake)\n",
        "\n",
        "    else:\n",
        "      target_fake = Variable(torch.ones(real_data_shape, 1)).float().to(device)\n",
        "\n",
        "      # D_loss_real = (D_result_from_real * target_real).mean()\n",
        "      D_loss_real = torch.mean(D_result_from_real)\n",
        "      # from fake는 fake데이터를 discriminator가 평가한것. \n",
        "      # 이것에 target_fake를 곱하면, \n",
        "      # D_loss_fake = (D_result_from_fake * target_fake).mean()\n",
        "      D_loss_fake = torch.mean(D_result_from_fake)\n",
        "\n",
        "    if activation_function == 'WGANGP':\n",
        "      gradient_penalty = compute_gradient_penalty(Discriminator, real_data.data, fake_data.data)\n",
        "    else:\n",
        "      gradient_penalty = torch.from_numpy(np.array([0])).to(device)\n",
        "\n",
        "    if activation_function == 'Sigmoid':\n",
        "      D_loss = D_loss_real + D_loss_fake\n",
        "    elif activation_function == 'WGAN':\n",
        "      D_loss = -1 * (D_loss_real - D_loss_fake)\n",
        "    elif activation_function == 'WGANGP':\n",
        "      D_loss = -1 * (D_loss_real - D_loss_fake) + lambda_gp * gradient_penalty\n",
        "\n",
        "    # Discriminator.zero_grad()\n",
        "    D_loss.backward()\n",
        "    D_optimizer.step()\n",
        "\n",
        "    # Clipping weights\n",
        "    # if activation_function == 'WGAN':\n",
        "    #   for temp_layer in Discriminator.discriminator_conv_layers:\n",
        "    #     if hasattr(temp_layer, 'weight'):\n",
        "    #       for temp_p in temp_layer02.parameters():\n",
        "    #         temp_p.data.clamp_(-CLIP_THRESHOLD, CLIP_THRESHOLD)\n",
        "    #   for temp_p in Discriminator.discriminator_flattened.parameters():\n",
        "    #     temp_p.data.clamp_(-CLIP_THRESHOLD, CLIP_THRESHOLD)\n",
        "    \n",
        "\n",
        "\n",
        "    if activation_function == 'Sigmoid':\n",
        "      batch_count_for_generator = 0\n",
        "\n",
        "    # Discriminator.train()\n",
        "    # Generator.train()\n",
        "\n",
        "    if batch_count_for_generator%5==0:\n",
        "      # print('generator',batch_count_for_generator)\n",
        "      for p in Discriminator.parameters():\n",
        "        p.requires_grad = False  # to avoid computation\n",
        "      for p in Generator.parameters():\n",
        "        p.requires_grad = True  # to avoid computation\n",
        "      G_optimizer.zero_grad()\n",
        "      Generator.zero_grad()\n",
        "      batch_count_for_generator = 0\n",
        "\n",
        "      ## 생성자\n",
        "\n",
        "      z = (torch.randn((real_data_shape, 100))).to(device)\n",
        "\n",
        "      fake_data = Generator(z)\n",
        "\n",
        "      D_result_from_fake = Discriminator(fake_data)\n",
        "\n",
        "      if activation_function == 'Sigmoid':\n",
        "        G_loss = criterion(D_result_from_fake, target_real)\n",
        "      else:\n",
        "        # G_loss = -1 * (D_result_from_fake * target_real).mean()\n",
        "        G_loss = -1 * torch.mean(D_result_from_fake)\n",
        "\n",
        "      G_loss.backward()\n",
        "      G_optimizer.step()\n",
        "\n",
        "      total_G_loss+=G_loss.item()\n",
        "\n",
        "    total_D_loss+=D_loss.item()\n",
        "    batch_count_for_generator+=1\n",
        "    \n",
        "    # print('\\t', D_loss.item())\n",
        "    # print('\\t', G_loss.item())\n",
        "\n",
        "  total_D_loss_numeric = round(total_D_loss,6)\n",
        "  total_G_loss_numeric = round(total_G_loss,6)\n",
        "\n",
        "  print_str = str(epoch) + ' ' + str(total_D_loss_numeric) + ' ' + str(total_G_loss_numeric)\n",
        "\n",
        "  print(print_str)\n",
        "\n",
        "  if epoch%10==0:\n",
        "    try:\n",
        "      torch.save(Discriminator.state_dict(), './drive/My Drive/Colab Notebooks/mlstudy/gan/gdl_code/trained_models/Discriminator_{}.pt'.format(activation_function))\n",
        "      torch.save(Generator.state_dict(), './drive/My Drive/Colab Notebooks/mlstudy/gan/gdl_code/trained_models/Generator_{}.pt'.format(activation_function))\n",
        "\n",
        "      Generator.eval()\n",
        "      for i in range(10):\n",
        "        generated_image = Generator(torch.randn([1,100]).to(device)).cpu().detach().numpy()[0].transpose(1,2,0)\n",
        "        generated_image02 = (((generated_image - generated_image.min())/((generated_image - generated_image.min()).max()))*255).astype(int)\n",
        "        img = Image.fromarray(np.uint8((generated_image02)))\n",
        "        img.save('./drive/My Drive/Colab Notebooks/mlstudy/gan/gdl_code/fig/image_{}_{}.png'.format(activation_function, i))\n",
        "      Generator.train()\n",
        "      \n",
        "    except:\n",
        "      pass"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 -396.355967 47.306708\n",
            "1 -169.334716 36.396272\n",
            "2 -233.969001 35.826734\n",
            "3 -410.738574 48.025122\n",
            "4 -724.77668 73.798481\n",
            "5 -947.564746 88.185338\n",
            "6 78.759503 80.88313\n",
            "7 16.816731 56.14915\n",
            "8 -42.39184 56.96152\n",
            "9 -68.791103 63.169453\n",
            "10 -97.939616 61.872069\n",
            "11 -125.486708 55.59266\n",
            "12 -173.485626 57.355412\n",
            "13 -210.25009 58.675388\n",
            "14 -204.294724 55.494898\n",
            "15 -11.446121 36.855808\n",
            "16 -24.364969 41.501018\n",
            "17 -44.130873 42.683252\n",
            "18 -58.642381 48.465553\n",
            "19 -50.118582 42.91917\n",
            "20 -46.737033 40.669711\n",
            "21 -66.035759 35.087871\n",
            "22 -40.79384 35.498149\n",
            "23 -58.229816 31.005201\n",
            "24 -78.142905 34.189741\n",
            "25 -70.70925 29.548682\n",
            "26 -78.035837 28.703191\n",
            "27 -88.476029 27.718298\n",
            "28 -85.27608 24.013015\n",
            "29 -82.762053 20.239281\n",
            "30 -88.798384 21.287295\n",
            "31 -89.818392 20.185746\n",
            "32 -93.874537 21.18656\n",
            "33 -100.438616 22.962656\n",
            "34 -95.892794 21.318297\n",
            "35 -80.893057 23.393411\n",
            "36 -6.874273 9.76158\n",
            "37 -24.41019 16.326273\n",
            "38 -29.372256 11.890822\n",
            "39 -40.825108 12.064406\n",
            "40 -33.756845 9.163119\n",
            "41 -30.693484 7.343691\n",
            "42 -29.786204 9.698134\n",
            "43 -20.956562 10.343006\n",
            "44 -24.778742 10.959949\n",
            "45 -19.958067 8.042518\n",
            "46 -21.157069 10.134543\n",
            "47 -22.203482 14.055577\n",
            "48 -19.088897 15.86498\n",
            "49 -19.381996 16.924612\n",
            "50 -14.503895 14.312283\n",
            "51 -14.612816 13.39853\n",
            "52 -15.059453 12.57939\n",
            "53 -13.1651 17.943667\n",
            "54 -13.308863 16.542281\n",
            "55 -12.094391 15.490909\n",
            "56 -15.986414 9.28221\n",
            "57 -19.473113 14.718419\n",
            "58 -16.245964 8.996952\n",
            "59 -13.361779 12.82794\n",
            "60 -18.634838 11.298455\n",
            "61 -13.423805 12.030932\n",
            "62 -16.469444 11.768219\n",
            "63 -11.937754 7.141663\n",
            "64 -16.668942 4.080717\n",
            "65 -14.953675 4.217729\n",
            "66 -17.061037 3.320456\n",
            "67 -11.726765 2.437785\n",
            "68 -15.826327 5.333552\n",
            "69 -17.011818 6.874403\n",
            "70 -14.266645 6.685299\n",
            "71 -10.272812 5.156932\n",
            "72 -17.447089 9.727338\n",
            "73 -14.707859 6.425441\n",
            "74 -14.553434 7.068003\n",
            "75 -14.028379 6.230097\n",
            "76 -16.126819 2.670666\n",
            "77 -15.741472 1.558659\n",
            "78 -14.694621 3.005053\n",
            "79 -16.167556 1.435176\n",
            "80 -13.884145 1.635204\n",
            "81 -14.43503 0.020946\n",
            "82 -15.357301 2.400386\n",
            "83 -15.060995 1.598609\n",
            "84 -16.268008 -1.62603\n",
            "85 -18.584994 -0.533796\n",
            "86 -15.509348 -2.303165\n",
            "87 -14.916283 -4.203336\n",
            "88 -18.421369 -5.759234\n",
            "89 -18.930412 -5.858673\n",
            "90 -18.637208 -7.570682\n",
            "91 -14.4635 -7.012342\n",
            "92 -15.780534 -5.354306\n",
            "93 -15.034009 -6.921815\n",
            "94 -15.487186 -2.674474\n",
            "95 -13.330109 -1.190175\n",
            "96 -16.876713 -1.451743\n",
            "97 -20.350776 -3.369482\n",
            "98 -18.185719 -4.301465\n",
            "99 -18.328588 -5.071427\n",
            "100 -14.595657 -5.238284\n",
            "101 -19.127634 -2.980317\n",
            "102 -16.425248 -7.239168\n",
            "103 -13.816531 -7.545017\n",
            "104 -17.28992 -5.761947\n",
            "105 -16.488441 -4.099581\n",
            "106 -16.848403 -3.749728\n",
            "107 -17.542162 -1.739617\n",
            "108 -18.173604 -3.418798\n",
            "109 -18.367459 -4.697786\n",
            "110 -13.385496 -1.696284\n",
            "111 -17.755225 -1.002208\n",
            "112 -16.917856 -1.860815\n",
            "113 -12.641026 -0.135367\n",
            "114 -19.669509 -0.530009\n",
            "115 -15.850999 -0.446579\n",
            "116 -17.297342 -1.705681\n",
            "117 -15.370909 -0.378636\n",
            "118 -16.320845 -2.982282\n",
            "119 -17.768432 1.289709\n",
            "120 -16.44806 1.914552\n",
            "121 -16.037402 0.560005\n",
            "122 -18.850131 -1.167443\n",
            "123 -20.701426 -2.31117\n",
            "124 -16.36858 -1.278933\n",
            "125 -15.672932 -3.151863\n",
            "126 -15.926115 -3.864837\n",
            "127 -14.991669 -4.395312\n",
            "128 -13.226997 -6.222401\n",
            "129 -15.576079 -6.038071\n",
            "130 -18.50938 -10.979351\n",
            "131 -13.542062 -9.645687\n",
            "132 -18.208383 -7.344404\n",
            "133 -12.942306 -8.659633\n",
            "134 -15.443747 -7.413229\n",
            "135 -17.295521 -5.421085\n",
            "136 -12.109734 -6.758275\n",
            "137 -15.281699 -5.109092\n",
            "138 -16.002392 -2.674295\n",
            "139 -17.453934 -6.414131\n",
            "140 -18.857487 -8.335497\n",
            "141 -14.477167 -8.112822\n",
            "142 -17.457932 -9.726608\n",
            "143 -17.776857 -6.456769\n",
            "144 -15.075136 -4.941535\n",
            "145 -13.274208 -7.843236\n",
            "146 -17.275676 -10.621693\n",
            "147 -15.536439 -7.458906\n",
            "148 -12.876604 -9.210123\n",
            "149 -18.023533 -10.131803\n",
            "150 -18.883955 -9.051926\n",
            "151 -16.272798 -7.368769\n",
            "152 -17.001212 -1.606126\n",
            "153 -19.449071 -3.495009\n",
            "154 -14.174144 -4.825731\n",
            "155 -22.076315 -3.572677\n",
            "156 -15.70876 -3.446438\n",
            "157 -15.217146 -3.80524\n",
            "158 -9.74855 -4.683405\n",
            "159 -17.56981 -3.978245\n",
            "160 -17.557611 -5.920507\n",
            "161 -14.256244 -11.901535\n",
            "162 -13.691119 -12.841749\n",
            "163 -11.605649 -12.714662\n",
            "164 -14.958093 -11.842109\n",
            "165 -17.103011 -4.932422\n",
            "166 -15.498197 -6.637768\n",
            "167 -16.18329 -9.821782\n",
            "168 -12.921518 -12.134393\n",
            "169 -15.772227 -8.089452\n",
            "170 -14.390505 -11.62479\n",
            "171 -9.582997 -7.335008\n",
            "172 -16.60325 -9.606257\n",
            "173 -14.29935 -12.56348\n",
            "174 -12.196924 -12.828905\n",
            "175 -12.736615 -10.201434\n",
            "176 -11.454383 -12.468312\n",
            "177 -15.191432 -13.699862\n",
            "178 -13.930758 -11.067225\n",
            "179 -17.007439 -11.749985\n",
            "180 -15.190576 -14.130867\n",
            "181 -11.741312 -13.00034\n",
            "182 -13.346365 -15.968076\n",
            "183 -14.615467 -14.353881\n",
            "184 -7.161084 -15.406181\n",
            "185 -12.697196 -12.989046\n",
            "186 -13.119921 -12.204571\n",
            "187 -5.261159 -12.4025\n",
            "188 -9.624607 -10.102365\n",
            "189 -13.451428 -13.400378\n",
            "190 -9.961568 -13.689063\n",
            "191 -11.565766 -14.682866\n",
            "192 -17.343895 -14.332928\n",
            "193 -14.034919 -11.688467\n",
            "194 -12.995511 -11.457478\n",
            "195 -12.041847 -13.240263\n",
            "196 -12.864059 -15.712932\n",
            "197 -13.480716 -16.610935\n",
            "198 -14.883937 -12.453337\n",
            "199 -16.117341 -13.13973\n",
            "200 -14.195036 -15.009893\n",
            "201 -10.823904 -14.48333\n",
            "202 -16.988107 -12.651741\n",
            "203 -12.61839 -12.429813\n",
            "204 -14.076458 -10.788463\n",
            "205 -9.937736 -9.469146\n",
            "206 -11.066271 -11.635019\n",
            "207 -10.559175 -11.302335\n",
            "208 -15.112961 -11.506772\n",
            "209 -14.368533 -11.296564\n",
            "210 -11.80064 -9.454807\n",
            "211 -8.288078 -8.040005\n",
            "212 -14.264815 -11.553764\n",
            "213 -14.66579 -12.479338\n",
            "214 -15.640444 -13.266083\n",
            "215 -18.644668 -17.471244\n",
            "216 -16.010236 -20.427278\n",
            "217 -14.800099 -16.618436\n",
            "218 -11.951009 -16.021649\n",
            "219 -13.259393 -14.394173\n",
            "220 -10.280122 -12.645543\n",
            "221 -19.057821 -9.758029\n",
            "222 -12.387875 -9.745255\n",
            "223 -14.943552 -10.221878\n",
            "224 -20.717328 -11.930168\n",
            "225 -15.038918 -12.868629\n",
            "226 -10.254693 -10.570059\n",
            "227 -8.854175 -9.501808\n",
            "228 -15.32607 -8.400908\n",
            "229 -17.524452 -8.491238\n",
            "230 -21.323361 -8.146515\n",
            "231 -13.530143 -5.614423\n",
            "232 -14.428857 -6.930676\n",
            "233 -11.946666 -6.944649\n",
            "234 -14.961147 -11.358098\n",
            "235 -10.212224 -8.159087\n",
            "236 -15.2162 -5.412154\n",
            "237 -17.60402 -5.1692\n",
            "238 -14.625603 -6.051019\n",
            "239 -14.848071 -5.277385\n",
            "240 -12.855953 -3.08089\n",
            "241 -13.772998 -6.075359\n",
            "242 -15.937322 -6.693348\n",
            "243 -16.742104 -3.73389\n",
            "244 -9.977966 -3.81194\n",
            "245 -9.096919 -4.849039\n",
            "246 -10.514071 -4.745711\n",
            "247 -15.26743 -3.311156\n",
            "248 -14.791616 -3.792\n",
            "249 -10.955227 -3.447961\n",
            "250 -15.324217 -4.142157\n",
            "251 -15.355822 -0.88292\n",
            "252 -17.092385 -1.110758\n",
            "253 -14.793129 -1.512494\n",
            "254 -13.109326 -1.976189\n",
            "255 -16.99643 -2.900045\n",
            "256 -17.448567 -6.810048\n",
            "257 -11.379066 -11.168174\n",
            "258 -11.53139 -8.999668\n",
            "259 -11.832725 -9.015685\n",
            "260 -14.172755 -7.280479\n",
            "261 -11.586641 -7.609467\n",
            "262 -12.738397 -9.590643\n",
            "263 -14.184975 -12.122329\n",
            "264 -13.907857 -10.499039\n",
            "265 -17.174083 -11.558157\n",
            "266 -13.745282 -9.29491\n",
            "267 -14.065621 -11.675882\n",
            "268 -15.319406 -10.134058\n",
            "269 -12.946448 -10.382344\n",
            "270 -11.892347 -11.715048\n",
            "271 -11.240406 -12.660653\n",
            "272 -15.593352 -14.269915\n",
            "273 -16.589403 -13.959164\n",
            "274 -12.597473 -15.272869\n",
            "275 -16.644417 -13.276108\n",
            "276 -19.062043 -16.086933\n",
            "277 -12.367831 -15.279732\n",
            "278 -15.916734 -12.682695\n",
            "279 -15.354055 -9.845276\n",
            "280 -13.031826 -10.293094\n",
            "281 -14.012639 -10.956407\n",
            "282 -15.08052 -11.36887\n",
            "283 -13.607009 -12.245731\n",
            "284 -13.568397 -12.111695\n",
            "285 -12.654354 -9.754222\n",
            "286 -15.680812 -9.776233\n",
            "287 -14.746969 -8.851655\n",
            "288 -18.652969 -6.799291\n",
            "289 -11.787671 -6.765202\n",
            "290 -13.428683 -6.975886\n",
            "291 -12.122411 -7.11647\n",
            "292 -14.876876 -7.388041\n",
            "293 -16.689558 -4.829334\n",
            "294 -15.339391 -7.251517\n",
            "295 -16.342944 -6.774251\n",
            "296 -15.987104 -4.525351\n",
            "297 -11.4864 -8.374358\n",
            "298 -16.982323 -6.492209\n",
            "299 -14.852586 -9.389799\n",
            "300 -12.448733 -7.965702\n",
            "301 -12.498741 -9.131927\n",
            "302 -13.260472 -6.465881\n",
            "303 -16.406093 -6.923203\n",
            "304 -18.954854 -6.075217\n",
            "305 -15.371658 -9.704623\n",
            "306 -14.930356 -11.729445\n",
            "307 -16.37575 -12.282761\n",
            "308 -11.077916 -8.766219\n",
            "309 -13.74364 -5.79381\n",
            "310 -16.19668 -7.183692\n",
            "311 -18.280191 -5.784202\n",
            "312 -16.124465 -8.642298\n",
            "313 -14.5987 -10.065244\n",
            "314 -12.598609 -9.621171\n",
            "315 -13.745438 -8.969339\n",
            "316 -12.010393 -10.621713\n",
            "317 -11.454151 -7.682819\n",
            "318 -13.807377 -8.018241\n",
            "319 -17.036756 -10.576083\n",
            "320 -12.0915 -11.017652\n",
            "321 -14.564154 -10.201299\n",
            "322 -18.055322 -6.539506\n",
            "323 -15.157566 -9.270481\n",
            "324 -15.472798 -10.674713\n",
            "325 -14.742215 -8.048755\n",
            "326 -17.678985 -7.833105\n",
            "327 -14.605733 -6.312762\n",
            "328 -13.883366 -6.556115\n",
            "329 -13.847288 -7.233352\n",
            "330 -13.324639 -8.985737\n",
            "331 -16.281556 -6.994303\n",
            "332 -15.280275 -7.567254\n",
            "333 -14.735391 -6.964212\n",
            "334 -11.141647 -10.715017\n",
            "335 -14.87722 -9.436523\n",
            "336 -18.065433 -7.512929\n",
            "337 -15.731438 -3.221744\n",
            "338 -16.193007 -5.682473\n",
            "339 -13.76991 -8.303908\n",
            "340 -12.600762 -6.979702\n",
            "341 -16.002205 -5.464506\n",
            "342 -16.598129 -4.666634\n",
            "343 -15.514253 -4.61364\n",
            "344 -17.260159 -2.976251\n",
            "345 -14.524454 -7.678543\n",
            "346 -15.193391 -5.20487\n",
            "347 -16.107266 -7.057611\n",
            "348 -14.59295 -5.492455\n",
            "349 -12.666913 -3.891071\n",
            "350 -14.951311 -1.683949\n",
            "351 -14.98004 -4.305103\n",
            "352 -15.253738 -4.034196\n",
            "353 -17.827575 -8.726297\n",
            "354 -14.961934 -11.412789\n",
            "355 -14.219515 -6.756348\n",
            "356 -16.039506 -7.765782\n",
            "357 -16.016193 -5.877611\n",
            "358 -12.746385 -5.836418\n",
            "359 -11.205118 -5.836897\n",
            "360 -11.843575 -7.23932\n",
            "361 -15.035324 -7.905669\n",
            "362 -11.298543 -6.576985\n",
            "363 -13.484321 -8.541369\n",
            "364 -14.670556 -8.269305\n",
            "365 -12.344193 -8.030217\n",
            "366 -11.848413 -9.454117\n",
            "367 -19.936426 -9.842485\n",
            "368 -11.443571 -7.075398\n",
            "369 -11.103131 -3.163211\n",
            "370 -16.875204 -1.456169\n",
            "371 -22.619259 -2.113752\n",
            "372 -12.691588 -4.608831\n",
            "373 -17.324985 -4.718267\n",
            "374 -20.68456 -3.593015\n",
            "375 -18.128675 -2.813316\n",
            "376 -13.28096 -6.107748\n",
            "377 -19.212528 -5.2319\n",
            "378 -13.039983 -5.082522\n",
            "379 -12.060662 -0.812033\n",
            "380 -16.671121 -5.223471\n",
            "381 -16.413491 -6.466195\n",
            "382 -16.276551 -7.747173\n",
            "383 -16.347905 -11.392376\n",
            "384 -14.257842 -8.813335\n",
            "385 -11.514054 -6.571108\n",
            "386 -12.848617 -8.174401\n",
            "387 -17.427653 -9.271144\n",
            "388 -20.824809 -5.958086\n",
            "389 -10.596251 -8.361039\n",
            "390 -13.674534 -6.734416\n",
            "391 -14.114072 -5.297951\n",
            "392 -13.998594 -5.519865\n",
            "393 -14.014095 -4.554763\n",
            "394 -14.212687 -6.665121\n",
            "395 -14.245486 -8.326163\n",
            "396 -14.983319 -5.469677\n",
            "397 -12.278925 -3.817687\n",
            "398 -13.687537 -1.281857\n",
            "399 -16.676334 -2.777256\n",
            "400 -18.446678 -4.348182\n",
            "401 -14.716793 -3.935034\n",
            "402 -14.499225 -0.685544\n",
            "403 -13.88954 0.271291\n",
            "404 -13.054068 -3.433016\n",
            "405 -14.594812 -4.944034\n",
            "406 -13.405753 -5.051514\n",
            "407 -13.393187 -6.433362\n",
            "408 -16.321898 -5.586914\n",
            "409 -15.605731 -2.449089\n",
            "410 -21.154295 -2.620681\n",
            "411 -12.877203 -4.586252\n",
            "412 -13.910906 -5.879452\n",
            "413 -12.376457 -4.056244\n",
            "414 -13.690365 -3.897269\n",
            "415 -15.296778 -3.561256\n",
            "416 -14.668862 -6.308159\n",
            "417 -15.070111 -7.57361\n",
            "418 -13.613887 -8.930728\n",
            "419 -16.554676 -11.516489\n",
            "420 -13.819964 -12.82406\n",
            "421 -12.784721 -13.045709\n",
            "422 -16.397417 -10.366602\n",
            "423 -12.126258 -14.065906\n",
            "424 -16.756041 -14.569952\n",
            "425 -16.807004 -14.620102\n",
            "426 -16.591709 -12.380337\n",
            "427 -14.080142 -10.710054\n",
            "428 -15.833879 -14.236271\n",
            "429 -13.570212 -11.406635\n",
            "430 -12.451773 -10.499967\n",
            "431 -17.350144 -15.116866\n",
            "432 -13.595696 -11.495399\n",
            "433 -17.105799 -10.446718\n",
            "434 -15.726343 -13.912957\n",
            "435 -13.537217 -13.572603\n",
            "436 -16.660946 -13.245243\n",
            "437 -16.232833 -13.908643\n",
            "438 -19.352311 -12.084324\n",
            "439 -13.933929 -12.012072\n",
            "440 -16.995916 -10.466742\n",
            "441 -16.205573 -13.023601\n",
            "442 -18.949492 -15.463593\n",
            "443 -14.905975 -13.761842\n",
            "444 -12.695128 -13.70874\n",
            "445 -13.006102 -12.777546\n",
            "446 -15.802928 -11.728743\n",
            "447 -14.53003 -11.465703\n",
            "448 -16.829832 -12.279553\n",
            "449 -17.985863 -8.78513\n",
            "450 -11.419894 -9.286152\n",
            "451 -15.750573 -8.867907\n",
            "452 -15.43533 -9.406939\n",
            "453 -17.481245 -10.445192\n",
            "454 -15.150919 -11.829444\n",
            "455 -14.593842 -9.773697\n",
            "456 -13.16887 -10.866852\n",
            "457 -9.957987 -12.764683\n",
            "458 -16.962975 -8.440416\n",
            "459 -17.357739 -7.068693\n",
            "460 -15.372869 -12.143657\n",
            "461 -14.628518 -11.430721\n",
            "462 -15.979426 -9.256139\n",
            "463 -15.202929 -11.510988\n",
            "464 -12.83892 -11.170421\n",
            "465 -10.925192 -9.621495\n",
            "466 -13.947765 -10.141767\n",
            "467 -14.690669 -7.411018\n",
            "468 -13.615514 -6.515752\n",
            "469 -19.751485 -9.232958\n",
            "470 -15.743497 -10.940777\n",
            "471 -13.318183 -11.739038\n",
            "472 -18.710292 -10.498432\n",
            "473 -12.689758 -6.89457\n",
            "474 -18.975912 -9.029862\n",
            "475 -11.642423 -12.754171\n",
            "476 -15.246331 -11.712797\n",
            "477 -17.070337 -10.653224\n",
            "478 -12.593418 -9.76138\n",
            "479 -14.70612 -9.718304\n",
            "480 -16.545579 -8.440327\n",
            "481 -15.072144 -9.126761\n",
            "482 -10.227158 -9.423788\n",
            "483 -13.022472 -10.543272\n",
            "484 -14.824432 -7.353109\n",
            "485 -14.454934 -10.391302\n",
            "486 -16.083472 -11.617032\n",
            "487 -13.05701 -9.939683\n",
            "488 -13.018853 -8.392491\n",
            "489 -17.234908 -9.947896\n",
            "490 -15.616381 -9.818924\n",
            "491 -13.763478 -9.880966\n",
            "492 -15.49202 -12.657027\n",
            "493 -15.760939 -12.446474\n",
            "494 -16.598675 -13.997679\n",
            "495 -15.534417 -11.601321\n",
            "496 -16.872564 -14.153565\n",
            "497 -14.824186 -11.978109\n",
            "498 -15.438798 -14.949889\n",
            "499 -13.500214 -13.398639\n",
            "500 -14.041429 -9.536562\n",
            "501 -16.413538 -12.135428\n",
            "502 -10.010239 -9.803333\n",
            "503 -12.562464 -11.974401\n",
            "504 -17.405328 -13.695807\n",
            "505 -15.664814 -12.051268\n",
            "506 -13.739602 -11.164415\n",
            "507 -14.733911 -11.969798\n",
            "508 -18.843443 -12.663254\n",
            "509 -14.289065 -12.809125\n",
            "510 -13.218116 -11.368854\n",
            "511 -17.120985 -10.328083\n",
            "512 -12.842093 -11.288858\n",
            "513 -14.460271 -12.18239\n",
            "514 -15.089446 -10.624166\n",
            "515 -14.391713 -13.712371\n",
            "516 -15.843683 -13.1693\n",
            "517 -13.762554 -12.510164\n",
            "518 -14.376961 -9.412509\n",
            "519 -15.024218 -8.085341\n",
            "520 -15.255083 -11.019852\n",
            "521 -13.957542 -12.223131\n",
            "522 -17.030655 -13.005212\n",
            "523 -18.755883 -11.519436\n",
            "524 -13.859758 -14.763677\n",
            "525 -13.524692 -12.750167\n",
            "526 -11.834287 -13.584986\n",
            "527 -14.441949 -11.732063\n",
            "528 -15.672435 -10.653348\n",
            "529 -18.003054 -10.329378\n",
            "530 -18.233687 -10.854582\n",
            "531 -12.027864 -11.75157\n",
            "532 -17.201292 -10.075734\n",
            "533 -16.258778 -11.57205\n",
            "534 -10.119627 -11.625705\n",
            "535 -10.665698 -11.485262\n",
            "536 -12.997718 -12.697309\n",
            "537 -14.131309 -12.568185\n",
            "538 -15.616279 -8.48934\n",
            "539 -15.027276 -10.559692\n",
            "540 -15.970358 -8.83633\n",
            "541 -15.249434 -10.466435\n",
            "542 -15.069621 -9.835999\n",
            "543 -11.633537 -9.883513\n",
            "544 -11.549639 -11.347134\n",
            "545 -16.438854 -13.071093\n",
            "546 -16.769805 -13.904262\n",
            "547 -14.230476 -12.685013\n",
            "548 -10.913379 -11.283836\n",
            "549 -12.795981 -11.765899\n",
            "550 -13.815838 -11.457323\n",
            "551 -12.337852 -10.022695\n",
            "552 -18.680482 -9.409302\n",
            "553 -12.60985 -10.586015\n",
            "554 -14.853605 -8.579886\n",
            "555 -16.651336 -9.967272\n",
            "556 -15.419246 -7.80527\n",
            "557 -14.492189 -8.743672\n",
            "558 -17.89749 -8.707613\n",
            "559 -17.541117 -7.826579\n",
            "560 -13.37525 -6.189897\n",
            "561 -18.025945 -8.394483\n",
            "562 -16.669721 -10.633856\n",
            "563 -10.89557 -11.862302\n",
            "564 -19.725852 -8.911637\n",
            "565 -17.212301 -8.691454\n",
            "566 -15.148722 -10.32619\n",
            "567 -13.343495 -8.484646\n",
            "568 -16.083085 -8.317583\n",
            "569 -12.532497 -10.966771\n",
            "570 -16.055681 -10.10669\n",
            "571 -13.21273 -12.288124\n",
            "572 -13.354494 -14.054738\n",
            "573 -15.932714 -13.757947\n",
            "574 -18.497565 -12.696448\n",
            "575 -18.17029 -14.169014\n",
            "576 -19.530001 -14.729445\n",
            "577 -16.155235 -15.938149\n",
            "578 -11.197142 -17.49702\n",
            "579 -13.730606 -14.840291\n",
            "580 -17.954848 -14.346945\n",
            "581 -15.191775 -13.260255\n",
            "582 -13.675672 -12.245867\n",
            "583 -14.543875 -11.634219\n",
            "584 -14.288372 -13.796523\n",
            "585 -14.5089 -15.876934\n",
            "586 -15.784061 -15.183728\n",
            "587 -18.600692 -15.947835\n",
            "588 -14.199548 -13.872023\n",
            "589 -17.548499 -13.798648\n",
            "590 -12.876972 -17.064349\n",
            "591 -16.059622 -15.82488\n",
            "592 -13.099447 -16.261814\n",
            "593 -13.641976 -14.109361\n",
            "594 -14.655765 -17.455784\n",
            "595 -14.841584 -15.600716\n",
            "596 -16.720767 -13.948325\n",
            "597 -16.51953 -12.756554\n",
            "598 -13.186314 -15.492501\n",
            "599 -14.086044 -14.946097\n",
            "600 -18.577593 -15.029208\n",
            "601 -14.058135 -18.218456\n",
            "602 -14.036848 -16.354145\n",
            "603 -12.623984 -15.294723\n",
            "604 -12.906422 -12.574048\n",
            "605 -17.530946 -10.421852\n",
            "606 -16.079717 -10.932009\n",
            "607 -15.610179 -9.643765\n",
            "608 -14.582171 -13.592143\n",
            "609 -11.167538 -12.540141\n",
            "610 -19.150379 -14.115468\n",
            "611 -15.675267 -15.515412\n",
            "612 -11.925239 -17.230371\n",
            "613 -13.984922 -13.865429\n",
            "614 -11.413344 -16.582198\n",
            "615 -20.029534 -14.277472\n",
            "616 -15.938748 -15.945639\n",
            "617 -15.853418 -15.914815\n",
            "618 -12.308139 -12.685067\n",
            "619 -14.796547 -14.013823\n",
            "620 -17.390778 -15.58557\n",
            "621 -14.278604 -13.286095\n",
            "622 -11.843449 -12.195892\n",
            "623 -13.357717 -13.517844\n",
            "624 -17.092867 -13.556872\n",
            "625 -15.745189 -15.668792\n",
            "626 -12.362836 -15.930446\n",
            "627 -16.112638 -15.104733\n",
            "628 -17.898034 -16.254279\n",
            "629 -10.324052 -12.5139\n",
            "630 -14.97244 -12.689373\n",
            "631 -17.524585 -13.506184\n",
            "632 -17.492752 -15.505551\n",
            "633 -13.692546 -14.045989\n",
            "634 -11.650241 -10.267183\n",
            "635 -12.037187 -10.882741\n",
            "636 -14.391738 -16.381957\n",
            "637 -17.280549 -13.82715\n",
            "638 -13.73404 -10.768639\n",
            "639 -15.775999 -10.342037\n",
            "640 -15.625833 -13.155166\n",
            "641 -15.345357 -12.284472\n",
            "642 -15.910374 -10.239799\n",
            "643 -11.393808 -10.100281\n",
            "644 -17.213031 -11.650034\n",
            "645 -14.485236 -12.211615\n",
            "646 -13.428018 -11.545225\n",
            "647 -15.680829 -12.684054\n",
            "648 -14.438159 -12.674447\n",
            "649 -14.570892 -9.467893\n",
            "650 -11.158205 -7.862811\n",
            "651 -15.70715 -10.433379\n",
            "652 -13.85784 -8.574622\n",
            "653 -12.855424 -7.133942\n",
            "654 -11.13078 -10.529871\n",
            "655 -13.649868 -10.440553\n",
            "656 -15.074563 -9.012607\n",
            "657 -17.164254 -8.618875\n",
            "658 -12.541287 -9.816735\n",
            "659 -12.276219 -10.15092\n",
            "660 -11.378225 -9.829639\n",
            "661 -11.556462 -10.067318\n",
            "662 -14.992983 -12.155223\n",
            "663 -14.798439 -11.496471\n",
            "664 -16.93112 -10.423482\n",
            "665 -17.623683 -9.686179\n",
            "666 -14.891041 -9.087192\n",
            "667 -13.19687 -12.317164\n",
            "668 -16.395113 -11.104382\n",
            "669 -13.686761 -8.901308\n",
            "670 -11.870963 -9.651533\n",
            "671 -15.800374 -12.240429\n",
            "672 -13.761063 -12.218186\n",
            "673 -11.360383 -9.052419\n",
            "674 -16.025615 -10.579205\n",
            "675 -11.44565 -9.822457\n",
            "676 -15.570089 -10.552234\n",
            "677 -15.760616 -11.062274\n",
            "678 -14.894951 -9.048096\n",
            "679 -12.712757 -8.30372\n",
            "680 -15.325652 -9.027519\n",
            "681 -14.442587 -9.259039\n",
            "682 -14.640958 -10.403055\n",
            "683 -14.05465 -8.504248\n",
            "684 -14.430398 -9.389271\n",
            "685 -14.281377 -8.151912\n",
            "686 -18.273151 -8.866075\n",
            "687 -13.82993 -7.880594\n",
            "688 -17.974614 -5.953617\n",
            "689 -14.347161 -6.418146\n",
            "690 -10.634024 -7.64694\n",
            "691 -15.267037 -8.085014\n",
            "692 -19.796734 -8.261815\n",
            "693 -12.466076 -7.095015\n",
            "694 -12.832497 -5.750365\n",
            "695 -13.845056 -6.616677\n",
            "696 -15.136989 -9.085641\n",
            "697 -19.716258 -9.238665\n",
            "698 -13.032743 -10.038226\n",
            "699 -16.848889 -10.185079\n",
            "700 -14.969227 -8.028809\n",
            "701 -13.933005 -10.683793\n",
            "702 -14.179352 -9.372256\n",
            "703 -19.41891 -8.434184\n",
            "704 -14.162998 -10.447551\n",
            "705 -11.760913 -9.844492\n",
            "706 -14.155106 -10.022167\n",
            "707 -16.883991 -12.80314\n",
            "708 -13.05077 -13.255831\n",
            "709 -13.78315 -16.174188\n",
            "710 -12.559562 -14.216136\n",
            "711 -13.353137 -11.984599\n",
            "712 -18.004578 -11.96793\n",
            "713 -12.343438 -10.827253\n",
            "714 -20.018459 -9.37146\n",
            "715 -15.732352 -11.843234\n",
            "716 -15.279248 -13.583628\n",
            "717 -15.283549 -14.091951\n",
            "718 -15.033426 -11.062132\n",
            "719 -14.898725 -10.602656\n",
            "720 -14.034337 -12.341089\n",
            "721 -12.655754 -11.832611\n",
            "722 -10.058113 -10.303641\n",
            "723 -13.908616 -8.20266\n",
            "724 -10.844416 -10.086767\n",
            "725 -15.354028 -9.040454\n",
            "726 -14.149903 -10.229779\n",
            "727 -14.889166 -9.420247\n",
            "728 -16.467319 -10.099353\n",
            "729 -14.759329 -8.273689\n",
            "730 -15.616756 -7.212598\n",
            "731 -13.635254 -9.574267\n",
            "732 -14.097078 -12.251689\n",
            "733 -11.522361 -10.340302\n",
            "734 -7.635305 -10.994353\n",
            "735 -15.594844 -11.271181\n",
            "736 -18.178927 -9.386334\n",
            "737 -17.473651 -12.043345\n",
            "738 -13.616116 -11.750621\n",
            "739 -11.887327 -10.780535\n",
            "740 -14.105977 -11.705701\n",
            "741 -13.091272 -9.554625\n",
            "742 -19.364124 -12.219053\n",
            "743 -10.357056 -10.445636\n",
            "744 -14.592242 -11.052433\n",
            "745 -17.911676 -12.168711\n",
            "746 -13.78989 -10.966944\n",
            "747 -16.352171 -11.014028\n",
            "748 -15.714188 -10.111952\n",
            "749 -17.129437 -11.037607\n",
            "750 -13.685269 -10.600911\n",
            "751 -11.260429 -12.003047\n",
            "752 -10.417535 -11.287208\n",
            "753 -14.048792 -8.823393\n",
            "754 -13.322276 -9.496526\n",
            "755 -16.382496 -10.653116\n",
            "756 -10.927664 -7.862376\n",
            "757 -11.428384 -7.279499\n",
            "758 -17.055464 -10.844178\n",
            "759 -17.344884 -6.658567\n",
            "760 -11.959612 -7.58347\n",
            "761 -14.972674 -9.75985\n",
            "762 -15.472435 -10.965397\n",
            "763 -13.319364 -9.667964\n",
            "764 -11.095583 -10.293765\n",
            "765 -10.354606 -13.105653\n",
            "766 -12.426195 -15.305144\n",
            "767 -13.028564 -14.492733\n",
            "768 -18.346473 -14.42759\n",
            "769 -13.235578 -15.133855\n",
            "770 -14.007419 -12.845222\n",
            "771 -16.555653 -13.612313\n",
            "772 -13.205327 -12.314577\n",
            "773 -8.423062 -13.186216\n",
            "774 -17.197869 -11.741966\n",
            "775 -12.550964 -11.747543\n",
            "776 -18.438499 -11.913704\n",
            "777 -16.05172 -13.156407\n",
            "778 -15.834029 -11.038531\n",
            "779 -19.751378 -7.327305\n",
            "780 -14.118619 -10.412316\n",
            "781 -8.874467 -10.199755\n",
            "782 -15.805662 -9.535597\n",
            "783 -19.366877 -7.944776\n",
            "784 -14.737489 -7.700885\n",
            "785 -13.473302 -8.981491\n",
            "786 -14.216243 -10.552914\n",
            "787 -17.129308 -6.469226\n",
            "788 -17.077923 -9.984328\n",
            "789 -18.508303 -9.665996\n",
            "790 -15.419992 -9.671698\n",
            "791 -14.79285 -10.92313\n",
            "792 -12.482118 -11.638085\n",
            "793 -14.146615 -11.960327\n",
            "794 -18.25785 -10.004307\n",
            "795 -16.552788 -7.915527\n",
            "796 -13.109598 -8.08818\n",
            "797 -14.924487 -7.620739\n",
            "798 -19.129363 -6.535783\n",
            "799 -10.478695 -9.63539\n",
            "800 -13.752321 -9.848625\n",
            "801 -10.793319 -7.99487\n",
            "802 -11.156703 -6.886095\n",
            "803 -19.647954 -7.719687\n",
            "804 -13.867081 -6.526281\n",
            "805 -14.117259 -7.800742\n",
            "806 -16.371799 -7.335075\n",
            "807 -17.826387 -10.715032\n",
            "808 -13.307713 -10.164968\n",
            "809 -14.392025 -8.163697\n",
            "810 -16.463445 -6.814988\n",
            "811 -13.111906 -10.832237\n",
            "812 -12.702955 -9.597989\n",
            "813 -17.373916 -12.644413\n",
            "814 -13.983472 -11.628938\n",
            "815 -10.358698 -10.68161\n",
            "816 -17.612724 -7.599243\n",
            "817 -16.733972 -10.626368\n",
            "818 -10.479332 -10.708114\n",
            "819 -13.500236 -9.486083\n",
            "820 -15.962366 -12.382604\n",
            "821 -11.040779 -13.06304\n",
            "822 -16.878927 -12.890738\n",
            "823 -17.199055 -12.048681\n",
            "824 -12.888996 -11.743613\n",
            "825 -15.01479 -10.415912\n",
            "826 -14.226528 -10.408119\n",
            "827 -15.008677 -10.614827\n",
            "828 -15.296225 -10.169115\n",
            "829 -12.408256 -8.956434\n",
            "830 -15.469746 -10.508832\n",
            "831 -12.202497 -10.154679\n",
            "832 -11.306772 -9.457802\n",
            "833 -14.301725 -10.804185\n",
            "834 -14.372451 -11.322789\n",
            "835 -18.461855 -11.001896\n",
            "836 -15.75892 -10.293993\n",
            "837 -14.752395 -9.324237\n",
            "838 -15.036869 -10.076227\n",
            "839 -10.696182 -9.573809\n",
            "840 -14.623653 -8.200814\n",
            "841 -13.811465 -7.446666\n",
            "842 -16.201988 -9.06672\n",
            "843 -15.438845 -8.828667\n",
            "844 -17.628119 -9.625935\n",
            "845 -14.453636 -7.891528\n",
            "846 -13.939462 -11.188518\n",
            "847 -18.639407 -9.453043\n",
            "848 -12.135874 -8.206267\n",
            "849 -11.588066 -7.240876\n",
            "850 -13.455783 -6.435889\n",
            "851 -9.577286 -6.219287\n",
            "852 -19.870263 -7.02875\n",
            "853 -17.784837 -8.005102\n",
            "854 -15.86675 -9.776906\n",
            "855 -12.881704 -10.303643\n",
            "856 -15.409356 -8.831374\n",
            "857 -15.036756 -9.859983\n",
            "858 -14.915982 -6.349253\n",
            "859 -16.217394 -5.724672\n",
            "860 -12.198334 -6.304494\n",
            "861 -15.835381 -7.264598\n",
            "862 -11.839062 -5.141853\n",
            "863 -15.596069 -6.001794\n",
            "864 -14.556407 -7.293799\n",
            "865 -14.8346 -8.953745\n",
            "866 -17.09763 -6.930065\n",
            "867 -10.755201 -8.618998\n",
            "868 -15.39638 -8.774638\n",
            "869 -11.2323 -8.8899\n",
            "870 -14.343998 -7.403661\n",
            "871 -14.657531 -4.090007\n",
            "872 -18.718053 -4.842253\n",
            "873 -11.404888 -9.342787\n",
            "874 -15.581898 -7.403228\n",
            "875 -14.248455 -6.575104\n",
            "876 -15.639436 -10.355967\n",
            "877 -16.585298 -10.212111\n",
            "878 -15.847242 -2.283021\n",
            "879 -12.600646 -3.885023\n",
            "880 -13.023266 -7.105631\n",
            "881 -17.554189 -10.11792\n",
            "882 -11.486104 -6.345527\n",
            "883 -16.205429 -4.133214\n",
            "884 -12.457813 -8.480124\n",
            "885 -13.625529 -8.6119\n",
            "886 -12.835361 -4.984619\n",
            "887 -13.889191 -4.973901\n",
            "888 -14.236484 -5.896578\n",
            "889 -13.620739 -7.22896\n",
            "890 -13.338769 -8.68372\n",
            "891 -15.017811 -8.221519\n",
            "892 -14.429864 -7.403368\n",
            "893 -13.62094 -7.517993\n",
            "894 -14.707093 -7.628997\n",
            "895 -13.518352 -5.306053\n",
            "896 -15.527061 -6.316214\n",
            "897 -8.643545 -5.931489\n",
            "898 -12.210132 -10.097188\n",
            "899 -19.005803 -7.367388\n",
            "900 -18.557664 -8.065071\n",
            "901 -14.77518 -7.481566\n",
            "902 -16.566723 -10.033162\n",
            "903 -11.012965 -13.260707\n",
            "904 -7.797886 -13.247847\n",
            "905 -17.367875 -11.177002\n",
            "906 -14.370691 -11.814189\n",
            "907 -11.677595 -9.61792\n",
            "908 -13.259489 -10.307825\n",
            "909 -13.044992 -10.008705\n",
            "910 -17.619974 -9.720722\n",
            "911 -15.375976 -11.116915\n",
            "912 -15.685925 -10.723353\n",
            "913 -15.095417 -7.981396\n",
            "914 -18.421855 -8.535976\n",
            "915 -11.940769 -8.356219\n",
            "916 -10.799679 -9.209579\n",
            "917 -15.960714 -9.439083\n",
            "918 -16.26778 -6.707343\n",
            "919 -15.305686 -11.156388\n",
            "920 -17.275316 -7.818153\n",
            "921 -20.066503 -4.741046\n",
            "922 -16.057313 -7.227064\n",
            "923 -16.106957 -8.695835\n",
            "924 -14.102284 -6.668792\n",
            "925 -15.167402 -5.622249\n",
            "926 -13.862278 -6.2863\n",
            "927 -10.49065 -8.550239\n",
            "928 -18.383234 -8.767997\n",
            "929 -16.801459 -9.919592\n",
            "930 -14.200432 -8.076753\n",
            "931 -16.332517 -8.814923\n",
            "932 -12.561714 -9.161578\n",
            "933 -17.726248 -6.453751\n",
            "934 -14.40095 -5.460147\n",
            "935 -16.471794 -6.291863\n",
            "936 -10.582085 -6.109188\n",
            "937 -11.18074 -7.50389\n",
            "938 -11.614887 -7.404467\n",
            "939 -17.176258 -9.405424\n",
            "940 -18.544493 -10.325414\n",
            "941 -13.39026 -7.53443\n",
            "942 -13.669124 -8.023759\n",
            "943 -15.153158 -6.026161\n",
            "944 -14.17175 -10.763003\n",
            "945 -14.340479 -11.283929\n",
            "946 -20.065924 -9.368176\n",
            "947 -14.469378 -8.224208\n",
            "948 -13.286258 -8.762261\n",
            "949 -14.278331 -7.048836\n",
            "950 -16.941378 -7.358329\n",
            "951 -15.904786 -7.070429\n",
            "952 -15.688508 -4.751884\n",
            "953 -15.489241 -5.673393\n",
            "954 -14.046881 -5.298204\n",
            "955 -13.741353 -6.281564\n",
            "956 -14.683913 -5.513101\n",
            "957 -13.24992 -4.032135\n",
            "958 -15.30526 -6.080838\n",
            "959 -17.228657 -9.475639\n",
            "960 -12.345245 -7.141166\n",
            "961 -17.801852 -7.493276\n",
            "962 -13.198128 -7.957414\n",
            "963 -17.216799 -6.968655\n",
            "964 -15.312665 -6.681235\n",
            "965 -15.396225 -8.18516\n",
            "966 -16.680645 -8.837164\n",
            "967 -14.553438 -8.244047\n",
            "968 -12.849446 -8.110854\n",
            "969 -17.170909 -9.364004\n",
            "970 -15.574547 -10.073608\n",
            "971 -17.186267 -11.988319\n",
            "972 -12.894832 -9.667028\n",
            "973 -16.993924 -9.741146\n",
            "974 -12.01012 -8.54855\n",
            "975 -14.038718 -6.960556\n",
            "976 -14.642261 -4.951838\n",
            "977 -11.598804 -8.218699\n",
            "978 -13.497344 -7.927157\n",
            "979 -14.38323 -8.088044\n",
            "980 -12.847457 -8.45609\n",
            "981 -16.077496 -9.666747\n",
            "982 -17.920289 -10.079841\n",
            "983 -14.813497 -9.474111\n",
            "984 -15.85939 -9.239627\n",
            "985 -10.66085 -8.562192\n",
            "986 -11.079471 -10.393468\n",
            "987 -11.981556 -8.950811\n",
            "988 -14.160821 -11.026953\n",
            "989 -16.946173 -9.843357\n",
            "990 -15.235294 -11.425716\n",
            "991 -17.577679 -12.216621\n",
            "992 -11.436552 -13.897694\n",
            "993 -18.33852 -9.051425\n",
            "994 -13.048274 -5.769749\n",
            "995 -11.554891 -8.73107\n",
            "996 -17.325697 -8.925276\n",
            "997 -11.145349 -6.507945\n",
            "998 -14.701555 -8.824668\n",
            "999 -13.876584 -10.686147\n",
            "1000 -16.173707 -8.03644\n",
            "1001 -16.931676 -7.839163\n",
            "1002 -15.569478 -9.452057\n",
            "1003 -14.578944 -7.324769\n",
            "1004 -17.002957 -5.222736\n",
            "1005 -11.624805 -8.200337\n",
            "1006 -15.327817 -5.065944\n",
            "1007 -16.490558 -10.400507\n",
            "1008 -10.842106 -8.917614\n",
            "1009 -15.057417 -7.776759\n",
            "1010 -11.750258 -6.474228\n",
            "1011 -15.967454 -4.187187\n",
            "1012 -13.732325 -8.510923\n",
            "1013 -12.91942 -8.361687\n",
            "1014 -17.929248 -8.62913\n",
            "1015 -15.541096 -8.07101\n",
            "1016 -10.826004 -7.219241\n",
            "1017 -13.971599 -7.422226\n",
            "1018 -16.550523 -3.724148\n",
            "1019 -10.715234 -6.33782\n",
            "1020 -16.797885 -9.148501\n",
            "1021 -13.801452 -7.586889\n",
            "1022 -14.572385 -9.48712\n",
            "1023 -14.331387 -7.682189\n",
            "1024 -13.208911 -9.948144\n",
            "1025 -16.498943 -10.423502\n",
            "1026 -13.668777 -8.503678\n",
            "1027 -17.281413 -8.988287\n",
            "1028 -12.730844 -3.839188\n",
            "1029 -14.11447 -7.259718\n",
            "1030 -14.04003 -7.688359\n",
            "1031 -14.278882 -4.711578\n",
            "1032 -16.016017 -6.347816\n",
            "1033 -10.887167 -6.586469\n",
            "1034 -11.987341 -4.830906\n",
            "1035 -15.563043 -3.995984\n",
            "1036 -14.988772 -3.593399\n",
            "1037 -13.249252 -3.795463\n",
            "1038 -16.574622 -6.179907\n",
            "1039 -14.374816 -5.490482\n",
            "1040 -12.171177 -4.495383\n",
            "1041 -15.584586 -5.493753\n",
            "1042 -15.586019 -6.301933\n",
            "1043 -14.833739 -2.999789\n",
            "1044 -15.016369 -4.021465\n",
            "1045 -15.405738 -3.629204\n",
            "1046 -13.180682 -6.13227\n",
            "1047 -11.526525 -5.860872\n",
            "1048 -14.291436 -4.919008\n",
            "1049 -16.549195 -3.06157\n",
            "1050 -13.982615 -4.631118\n",
            "1051 -12.756648 -6.159973\n",
            "1052 -14.601707 -5.651084\n",
            "1053 -15.041126 -7.423091\n",
            "1054 -14.026531 -6.76129\n",
            "1055 -13.5802 -3.747408\n",
            "1056 -14.116876 -5.65152\n",
            "1057 -15.970504 -6.995427\n",
            "1058 -9.373028 -4.558046\n",
            "1059 -14.658625 -5.224684\n",
            "1060 -16.49232 -2.041781\n",
            "1061 -15.432854 -4.163995\n",
            "1062 -11.936261 -3.649466\n",
            "1063 -15.932535 -1.763725\n",
            "1064 -12.184763 -1.997645\n",
            "1065 -12.606841 -4.74636\n",
            "1066 -16.848955 -5.693632\n",
            "1067 -16.050428 -7.809019\n",
            "1068 -14.055722 -7.061957\n",
            "1069 -15.458695 -6.026428\n",
            "1070 -14.587835 -3.661626\n",
            "1071 -17.037327 -1.250233\n",
            "1072 -16.380956 -5.625033\n",
            "1073 -14.748902 -2.260483\n",
            "1074 -12.190075 -3.027446\n",
            "1075 -18.49567 -5.505853\n",
            "1076 -11.944819 -4.27924\n",
            "1077 -13.955925 -5.846515\n",
            "1078 -14.919176 -6.303072\n",
            "1079 -14.900774 -4.401335\n",
            "1080 -12.373871 -7.203938\n",
            "1081 -19.442177 -6.023527\n",
            "1082 -15.460646 -4.331831\n",
            "1083 -17.551606 -4.902316\n",
            "1084 -13.807334 -6.18667\n",
            "1085 -15.540756 -6.367285\n",
            "1086 -10.172783 -7.191494\n",
            "1087 -15.592971 -4.714477\n",
            "1088 -17.759202 -5.600297\n",
            "1089 -15.035232 -5.523222\n",
            "1090 -18.518964 -7.013324\n",
            "1091 -17.728757 -7.445693\n",
            "1092 -11.784775 -7.229868\n",
            "1093 -14.746683 -9.76287\n",
            "1094 -14.406074 -9.630357\n",
            "1095 -16.214249 -11.14929\n",
            "1096 -13.890399 -8.101296\n",
            "1097 -14.752232 -9.903975\n",
            "1098 -15.534211 -9.724526\n",
            "1099 -17.146786 -5.951607\n",
            "1100 -17.526551 -5.797061\n",
            "1101 -16.341267 -9.273656\n",
            "1102 -14.478546 -6.137903\n",
            "1103 -10.387301 -4.800655\n",
            "1104 -13.824784 -5.696308\n",
            "1105 -13.415693 -7.266451\n",
            "1106 -15.60685 -7.95076\n",
            "1107 -12.490869 -6.434334\n",
            "1108 -16.905567 -6.11525\n",
            "1109 -16.347533 -9.256313\n",
            "1110 -15.076618 -6.737276\n",
            "1111 -19.053521 -6.089692\n",
            "1112 -14.278183 -6.511348\n",
            "1113 -18.035186 -5.94541\n",
            "1114 -15.505465 -4.844187\n",
            "1115 -13.12455 -5.97497\n",
            "1116 -17.031476 -7.858633\n",
            "1117 -18.327254 -8.469977\n",
            "1118 -12.554747 -6.769818\n",
            "1119 -15.668225 -6.367188\n",
            "1120 -13.516152 -7.171272\n",
            "1121 -15.784517 -8.266085\n",
            "1122 -13.955073 -8.926004\n",
            "1123 -17.958052 -9.5168\n",
            "1124 -14.484541 -9.695057\n",
            "1125 -17.831462 -10.378788\n",
            "1126 -16.773123 -9.469294\n",
            "1127 -16.554783 -9.438019\n",
            "1128 -14.455268 -6.596926\n",
            "1129 -18.568468 -1.434974\n",
            "1130 -15.169814 -7.535424\n",
            "1131 -16.665723 -9.341443\n",
            "1132 -12.982942 -8.731852\n",
            "1133 -14.558795 -8.912343\n",
            "1134 -17.079951 -6.314501\n",
            "1135 -15.668868 -7.092308\n",
            "1136 -15.904059 -9.855585\n",
            "1137 -16.995533 -7.580176\n",
            "1138 -18.618811 -6.356255\n",
            "1139 -13.580688 -5.26938\n",
            "1140 -15.951125 -5.766227\n",
            "1141 -14.045607 -6.22181\n",
            "1142 -15.44539 -6.853845\n",
            "1143 -12.82998 -6.635894\n",
            "1144 -14.432001 -5.103154\n",
            "1145 -17.050614 -1.847299\n",
            "1146 -16.540739 -2.633014\n",
            "1147 -14.585985 -4.530336\n",
            "1148 -18.850149 -5.1678\n",
            "1149 -15.997277 -4.959011\n",
            "1150 -15.602976 -6.175277\n",
            "1151 -13.676353 -6.041881\n",
            "1152 -15.193254 -6.978875\n",
            "1153 -8.468093 -5.359516\n",
            "1154 -16.242322 -7.872821\n",
            "1155 -17.355581 -5.130672\n",
            "1156 -15.442721 -5.536162\n",
            "1157 -10.624165 -5.175059\n",
            "1158 -13.382365 -3.197056\n",
            "1159 -17.939852 -2.483278\n",
            "1160 -13.916949 -3.047276\n",
            "1161 -12.347025 -1.868567\n",
            "1162 -19.296443 -2.815213\n",
            "1163 -18.944032 -2.466811\n",
            "1164 -15.193086 -4.895025\n",
            "1165 -13.367379 -4.241793\n",
            "1166 -15.209712 -2.488195\n",
            "1167 -16.096662 -2.329706\n",
            "1168 -15.272246 -1.873094\n",
            "1169 -16.702892 -1.733287\n",
            "1170 -17.619166 -0.268057\n",
            "1171 -13.300861 -2.549654\n",
            "1172 -17.534503 -3.141319\n",
            "1173 -13.37145 -5.265898\n",
            "1174 -15.752028 -1.88243\n",
            "1175 -15.379756 -1.078225\n",
            "1176 -17.327598 -2.786356\n",
            "1177 -19.130695 -3.878889\n",
            "1178 -8.994329 -0.474537\n",
            "1179 -13.494375 -1.481658\n",
            "1180 -19.781259 -3.735411\n",
            "1181 -16.961814 -0.005377\n",
            "1182 -16.086506 -0.046556\n",
            "1183 -17.901535 0.463356\n",
            "1184 -15.824171 1.421022\n",
            "1185 -15.881057 1.143663\n",
            "1186 -11.813078 1.494448\n",
            "1187 -14.777654 -0.443058\n",
            "1188 -17.902679 1.617881\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "caSmwvfCI4KK",
        "colab_type": "text"
      },
      "source": [
        "## GAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "r1Mv0S8hcUL5",
        "colab": {}
      },
      "source": [
        "# Generator.eval()\n",
        "# generated_image = Generator(torch.randn([1,100]).to(device)).cpu().detach().numpy()[0].transpose(1,2,0)\n",
        "# generated_image02 = (((generated_image - generated_image.min())/((generated_image - generated_image.min()).max()))*255).astype(int)\n",
        "# img = Image.fromarray(np.uint8((generated_image02)))\n",
        "# Generator.train()\n",
        "# try:\n",
        "#   img.save('./drive/My Drive/Colab Notebooks/mlstudy/gan/gdl_code/fig/image.png')\n",
        "# except:\n",
        "#   pass"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mdutjRq6aZsN",
        "colab_type": "code",
        "outputId": "ddf8236d-3bb0-43a1-d63d-d40b84d2dbaf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        }
      },
      "source": [
        "Generator.eval()\n",
        "plt.imshow(Generator(torch.randn([1,100]).to(device)).cpu().detach().numpy()[0].transpose(1,2,0))\n",
        "Generator.train()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f95445d1710>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 665
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD5CAYAAADhukOtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k-5gSSF3I6yz",
        "colab_type": "text"
      },
      "source": [
        "## WGAN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmqTocPZI6g6",
        "colab_type": "code",
        "outputId": "8a67c197-8ed6-4b91-9531-3cc318a28f5b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        }
      },
      "source": [
        "Generator.eval()\n",
        "plt.imshow(Generator(torch.randn([1,100]).to(device)).cpu().detach().numpy()[0].transpose(1,2,0))\n",
        "Generator.train()\n",
        "print(1)"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD5CAYAAADhukOtAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngl2ISFCwCxEJCnYhIuH/AmReoxgCYv1YAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nY4M8LLpI9Iz",
        "colab_type": "text"
      },
      "source": [
        "## WGAN-GP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KZihgzdn4MFq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "Generator.eval()\n",
        "plt.imshow(Generator(torch.randn([1,100]).to(device)).cpu().detach().numpy()[0].transpose(1,2,0))\n",
        "Generator.train()\n",
        "print(1)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}